{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import environment\n",
    "import job_distribution\n",
    "import slow_down_cdf\n",
    "import RL_brain\n",
    "import parameters\n",
    "from gurobipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Given vector x, computes a vector y such that\n",
    "    y[i] = x[i] + gamma * x[i+1] + gamma^2 x[i+2] + ...\n",
    "    \"\"\"\n",
    "    out = np.zeros(len(x))\n",
    "    out[-1] = x[-1]\n",
    "    for i in reversed(range(len(x)-1)):\n",
    "        out[i] = x[i] + gamma*out[i+1]\n",
    "    assert x.ndim >= 1\n",
    "    # More efficient version:\n",
    "    # scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(agent, env, episode_max_length):\n",
    "    \"\"\"\n",
    "    Run agent-environment loop for one whole episode (trajectory)\n",
    "    Return dictionary of results\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    obs = []\n",
    "    acts = []\n",
    "    rews = []\n",
    "    info = []\n",
    "\n",
    "    ob = env.observe()\n",
    "\n",
    "    for _ in range(episode_max_length):\n",
    "    #for _ in range(5):\n",
    "\n",
    "        loss = 0\n",
    "        #print('ob_len:',len(ob[0]))\n",
    "        a = agent.choose_action(ob)\n",
    "\n",
    "        obs.append(ob)  # store the ob at current decision making step\n",
    "        acts.append(a)\n",
    "\n",
    "        ob_, rew, done, info = env.step(a, repeat=True)\n",
    "\n",
    "        # agent.store_transition(ob, a, rew)\n",
    "\n",
    "        rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "\n",
    "            # loss = agent.learn()\n",
    "            break\n",
    "\n",
    "        ob = ob_\n",
    "\n",
    "    # loss = agent.learn()\n",
    "\n",
    "    return {'reward': np.array(rews),\n",
    "            'ob': np.array(obs),\n",
    "            'action': np.array(acts),\n",
    "            'info': info,\n",
    "            # 'loss': loss\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_ob(trajs, pa):\n",
    "\n",
    "    timesteps_total = 0\n",
    "    for i in range(len(trajs)):\n",
    "        timesteps_total += len(trajs[i]['reward'])\n",
    "\n",
    "    all_ob = np.zeros(\n",
    "        (timesteps_total, pa.network_input_height*pa.network_input_width),\n",
    "        dtype=np.float64)\n",
    "\n",
    "    timesteps = 0\n",
    "    for i in range(len(trajs)):\n",
    "        for j in range(len(trajs[i]['reward'])):\n",
    "            all_ob[timesteps, :] = trajs[i]['ob'][j]\n",
    "            timesteps += 1\n",
    "\n",
    "    return all_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_info(trajs):\n",
    "    enter_time = []\n",
    "    finish_time = []\n",
    "    job_len = []\n",
    "\n",
    "    for traj in trajs:\n",
    "        enter_time.append(np.array([traj['info'].record[i].enter_time for i in range(len(traj['info'].record))]))\n",
    "        finish_time.append(np.array([traj['info'].record[i].finish_time for i in range(len(traj['info'].record))]))\n",
    "        job_len.append(np.array([traj['info'].record[i].len for i in range(len(traj['info'].record))]))\n",
    "\n",
    "    enter_time = np.concatenate(enter_time)\n",
    "    finish_time = np.concatenate(finish_time)\n",
    "    job_len = np.concatenate(job_len)\n",
    "\n",
    "    return enter_time, finish_time, job_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_curve(output_file_prefix, max_rew_lr_curve, mean_rew_lr_curve, slow_down_lr_curve,\n",
    "                  ref_discount_rews, ref_slow_down,ref_lr_gurobi=None):\n",
    "    num_colors = len(ref_discount_rews) + 2\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    #ax.set_prop_cycle([cm(1. * i / num_colors) for i in range(num_colors)])\n",
    "    ax.set_prop_cycle(color=['blue', 'green', 'orange', 'red','yellow','gray'])\n",
    "    ax.plot(mean_rew_lr_curve, linewidth=2, label='PG mean')\n",
    "    ax.plot(max_rew_lr_curve, linewidth=2, label='PG max')\n",
    "    for k in ref_discount_rews:\n",
    "        ax.plot(np.tile(np.average(ref_discount_rews[k]), len(mean_rew_lr_curve)), linewidth=2, label=k)\n",
    "    plt.legend(loc=4)\n",
    "    plt.xlabel(\"Iteration\", fontsize=20)\n",
    "    plt.ylabel(\"Discounted Total Reward\", fontsize=20)\n",
    "\n",
    "    \n",
    "    \n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_prop_cycle(color=['blue', 'green', 'orange', 'red','yellow','gray'])\n",
    "    #ax.set_color_cycle([cm(1. * i / num_colors) for i in range(num_colors)])\n",
    "\n",
    "    ax.plot(slow_down_lr_curve, linewidth=2, label='PG mean')\n",
    "    #######todo：添加gurobi计算结果####\n",
    "    #ax.plot(ref_lr_gurobi, linewidth=2, label='Gurobi')\n",
    "    ###########################\n",
    "    for k in ref_discount_rews:\n",
    "        ax.plot(np.tile(np.average(np.concatenate(ref_slow_down[k])), len(slow_down_lr_curve)), linewidth=2, label=k)\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel(\"Iteration\", fontsize=20)\n",
    "    plt.ylabel(\"Completion time\", fontsize=20)\n",
    "\n",
    "    plt.savefig(output_file_prefix + \"_lr_curve\" + \".pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_worker(rl, env, pa):\n",
    "\n",
    "    trajs = []\n",
    "\n",
    "    for i in range(pa.num_seq_per_batch):\n",
    "        traj = get_traj(rl, env, pa.episode_max_length)\n",
    "        trajs.append(traj)\n",
    "\n",
    "    all_ob = concatenate_all_ob(trajs, pa)\n",
    "\n",
    "    # Compute discounted sums of rewards\n",
    "    rets = [discount(traj[\"reward\"], pa.discount) for traj in trajs]\n",
    "    maxlen = max(len(ret) for ret in rets)\n",
    "    padded_rets = [np.concatenate([ret, np.zeros(maxlen - len(ret))]) for ret in rets]\n",
    "\n",
    "    # Compute time-dependent baseline\n",
    "    baseline = np.mean(padded_rets, axis=0)\n",
    "\n",
    "    # Compute advantage function\n",
    "    advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "    all_action = np.concatenate([traj[\"action\"] for traj in trajs])\n",
    "    all_adv = np.concatenate(advs)\n",
    "\n",
    "    all_eprews = np.array([discount(traj[\"reward\"], pa.discount)[0] for traj in trajs])  # episode total rewards\n",
    "    all_eplens = np.array([len(traj[\"reward\"]) for traj in trajs])  # episode lengths\n",
    "    # all_loss = np.array([traj[\"loss\"] for traj in trajs])\n",
    "\n",
    "    # All Job Stat\n",
    "    enter_time, finish_time, job_len = process_all_info(trajs)\n",
    "    finished_idx = (finish_time >= 0)\n",
    "    completion_time = finish_time[finished_idx] - enter_time[finished_idx]\n",
    "\n",
    "    return all_eprews, all_eplens, completion_time, all_ob, all_action, all_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for workers...\n",
      "-prepare for env- 0\n",
      "-prepare for env- 1\n",
      "-prepare for env- 2\n",
      "-prepare for env- 3\n",
      "-prepare for env- 4\n",
      "-prepare for env- 5\n",
      "-prepare for env- 6\n",
      "-prepare for env- 7\n",
      "-prepare for env- 8\n",
      "-prepare for env- 9\n",
      "-prepare for worker-\n",
      "Preparing for reference data...\n",
      "Load on # 0 resource dimension is 2.8845\n",
      "Load on # 1 resource dimension is 3.44875\n",
      "\n",
      "\n",
      "\n",
      "=============== 0 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -2632.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1144.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -2534.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2108.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 1 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -2508.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1574.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -2339.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2447.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 2 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -1763.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1130.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -1508.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2290.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 3 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -2483.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1464.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -2319.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2709.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 4 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -1828.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -895.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -1625.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2138.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 5 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -913.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -428.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -880.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -1473.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 6 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -1719.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -938.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -1956.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2429.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 7 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -2541.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1156.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -2416.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -1948.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 8 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -1711.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1122.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -1686.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2009.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 9 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -2002.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -1183.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -1924.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -2341.0\n",
      "Start training...\n",
      "\n",
      "\n",
      "\n",
      "-----------------\n",
      "Iteration: \t 1\n",
      "NumTrajs: \t 10\n",
      "NumTimesteps: \t 46248\n",
      "Loss:     \t -142.16356\n",
      "MaxRew: \t -2173.6\n",
      "MeanRew: \t -2738.09 +- 614.1057090599305\n",
      "MeanSlowdown: \t 68.42724999999999\n",
      "MeanLen: \t 462.48 +- 213.4534834571692\n",
      "Elapsed time\t 30.631352186203003 seconds\n",
      "-----------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------\n",
      "Iteration: \t 2\n",
      "NumTrajs: \t 10\n",
      "NumTimesteps: \t 45375\n",
      "Loss:     \t -147.30478\n",
      "MaxRew: \t -2106.1\n",
      "MeanRew: \t -2799.49 +- 672.0424911417432\n",
      "MeanSlowdown: \t 69.962\n",
      "MeanLen: \t 453.75 +- 217.03729518218753\n",
      "Elapsed time\t 30.19704294204712 seconds\n",
      "-----------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------\n",
      "Iteration: \t 3\n",
      "NumTrajs: \t 10\n",
      "NumTimesteps: \t 39167\n",
      "Loss:     \t -66.08021\n",
      "MaxRew: \t -2111.2\n",
      "MeanRew: \t -2613.53 +- 622.1393325453713\n",
      "MeanSlowdown: \t 65.30825\n",
      "MeanLen: \t 391.67 +- 145.17300403311904\n",
      "Elapsed time\t 26.328066110610962 seconds\n",
      "-----------------\n",
      "\n",
      "\n",
      "\n",
      "-----------------\n",
      "Iteration: \t 4\n",
      "NumTrajs: \t 10\n",
      "NumTimesteps: \t 39563\n",
      "Loss:     \t -142.37558\n",
      "MaxRew: \t -1976.8\n",
      "MeanRew: \t -2544.88 +- 660.4472315030172\n",
      "MeanSlowdown: \t 63.59174999999999\n",
      "MeanLen: \t 395.63 +- 197.0457132241146\n",
      "Elapsed time\t 26.44086790084839 seconds\n",
      "-----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9d975d5835e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9d975d5835e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mrender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all_done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9d975d5835e4>\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(pa, pg_resume, render, repre, end)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mex_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0meprew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meplen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_traj_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0meprewlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meprew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0meplenlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meplen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8af0c8e6e896>\u001b[0m in \u001b[0;36mget_traj_worker\u001b[0;34m(rl, env, pa)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_seq_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_traj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrajs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d6047c8874ef>\u001b[0m in \u001b[0;36mget_traj\u001b[0;34m(agent, env, episode_max_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print('ob_len:',len(ob[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# store the ob at current decision making step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Diplomarbeit/PG_for_cloud/PG for completion time/RL_brain.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mprob_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_act_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_obs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m#print('prob:',prob_weights)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# select action w.r.t the actions prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;31m#print('action:',action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/getlimits.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0m_finfo_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def launch(pa, pg_resume=None, render=False, repre='image', end='no_new_job'):\n",
    "\n",
    "    # ----------------------------\n",
    "    print(\"Preparing for workers...\")\n",
    "    # ----------------------------\n",
    "\n",
    "    pg_learners = []\n",
    "    envs = []\n",
    "\n",
    "    nw_len_seqs, nw_size_seqs = job_distribution.generate_sequence_work(pa, seed=42)#生成一序列的任务，其中包括num_ex个task\n",
    "\n",
    "    for ex in range(pa.num_ex):#对于每个task\n",
    "\n",
    "        print(\"-prepare for env-\", ex)\n",
    "\n",
    "        env = environment.Env(pa, nw_len_seqs=nw_len_seqs, nw_size_seqs=nw_size_seqs,\n",
    "                              render=False, repre=repre, end=end)#初始化一个环境\n",
    "        env.seq_no = ex\n",
    "        envs.append(env)\n",
    "\n",
    "    print(\"-prepare for worker-\")\n",
    "\n",
    "    rl = RL_brain.PolicyGradient(n_actions=pa.network_output_dim,\n",
    "                                 network_input_height=pa.network_input_height,\n",
    "                                 network_input_width=pa.network_input_width,\n",
    "                                 n_features=pa.network_input_height*pa.network_input_width,\n",
    "                                 learning_rate=0.002)\n",
    "\n",
    "\n",
    "    if pg_resume is not None:\n",
    "        rl.load_data(pg_resume)\n",
    "\n",
    "\n",
    "    # --------------------------------------\n",
    "    print(\"Preparing for reference data...\")\n",
    "    # --------------------------------------\n",
    "\n",
    "    ref_discount_rews, ref_slow_down = slow_down_cdf.launch(pa, pg_resume=None,render=True,\n",
    "                                                            plot=False, repre=repre, end=end)\n",
    "    mean_rew_lr_curve = []\n",
    "    max_rew_lr_curve = []\n",
    "    slow_down_lr_curve = []\n",
    "\n",
    "    # --------------------------------------\n",
    "    print(\"Start training...\")\n",
    "    # --------------------------------------\n",
    "\n",
    "    timer_start = time.time()\n",
    "    \n",
    "    ref_lr_gurobi=[]\n",
    "\n",
    "    for iteration in range(1, pa.num_epochs):#进行每一次迭代\n",
    "    #for iteration in range(1, 2):\n",
    "\n",
    "        ex_indices = list(range(pa.num_ex))\n",
    "        np.random.shuffle(ex_indices)#打乱每一次的所有task\n",
    "\n",
    "        all_eprews = []#所有迭代次数的rewards\n",
    "        eprews = []#每次迭代的总rewards\n",
    "        eplens = []#每次迭代完成所有任务的总时长\n",
    "        all_slowdown = []#所有迭代的总slowdown\n",
    "\n",
    "        eprewlist = []\n",
    "        eplenlist =[]\n",
    "        slowdownlist =[]\n",
    "        losslist = []\n",
    "        \n",
    "\n",
    "        ex_counter = 0\n",
    "        for ex in range(pa.num_ex):\n",
    "\n",
    "            ex_idx = ex_indices[ex]\n",
    "\n",
    "            eprew, eplen, completion_time, all_ob, all_action, all_adv = get_traj_worker(rl, envs[ex_idx], pa)\n",
    "            eprewlist.append(eprew)\n",
    "            eplenlist.append(eplen)\n",
    "            slowdownlist.append(completion_time)\n",
    "            \n",
    "            \n",
    "            loss = rl.learn(all_ob, all_action, all_adv)\n",
    "            losslist.append(loss)\n",
    "\n",
    "            ex_counter += 1\n",
    "\n",
    "            if ex_counter >= pa.batch_size or ex == pa.num_ex - 1:\n",
    "\n",
    "                print(\"\\n\\n\")\n",
    "\n",
    "                ex_counter = 0\n",
    "\n",
    "               \n",
    "\n",
    "        timer_end = time.time()\n",
    "\n",
    "        print(\"-----------------\")\n",
    "        print(\"Iteration: \\t %i\" % iteration)\n",
    "        print(\"NumTrajs: \\t %i\" % len(eprewlist))\n",
    "        print(\"NumTimesteps: \\t %i\" % np.sum(eplenlist))\n",
    "        print(\"Loss:     \\t %s\" % np.mean(losslist))\n",
    "        print(\"MaxRew: \\t %s\" % np.average([np.max(rew) for rew in eprewlist]))\n",
    "        print(\"MeanRew: \\t %s +- %s\" % (np.mean(eprewlist), np.std(eprewlist)))\n",
    "        print(\"MeanSlowdown: \\t %s\" % np.mean([np.mean(sd) for sd in slowdownlist]))\n",
    "        print(\"MeanLen: \\t %s +- %s\" % (np.mean(eplenlist), np.std(eplenlist)))\n",
    "        print(\"Elapsed time\\t %s\" % (timer_end - timer_start), \"seconds\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        timer_start = time.time()\n",
    "\n",
    "        max_rew_lr_curve.append(np.average([np.max(rew) for rew in eprewlist]))\n",
    "        mean_rew_lr_curve.append(np.mean(eprewlist))\n",
    "        slow_down_lr_curve.append(np.mean([np.mean(sd) for sd in slowdownlist]))\n",
    "        \n",
    "        #slowdown_gurobi=ref_gurobi(pa)\n",
    "        #ref_lr_gurobi.append(slowdown_gurobi)\n",
    "        \n",
    "        \n",
    "        if iteration % pa.output_freq == 0:\n",
    "\n",
    "            rl.save_data(pa.output_filename + '_' + str(iteration))\n",
    "\n",
    "            pa.unseen = True\n",
    "            slow_down_cdf.launch(pa, pa.output_filename + '_' + str(iteration) + '.ckpt',render=False, plot=True, repre=repre, end=end)\n",
    "            pa.unseen = False\n",
    "            #print(slow_down_lr_curve)\n",
    "            #mean_lr_gurobi=np.mean(ref_lr_gurobi)\n",
    "            #for i in range(len(ref_lr_gurobi)):\n",
    "                #ref_lr_gurobi[i]=mean_lr_gurobi\n",
    "\n",
    "            plot_lr_curve(pa.output_filename,\n",
    "                          max_rew_lr_curve, mean_rew_lr_curve, slow_down_lr_curve,\n",
    "                          ref_discount_rews, ref_slow_down,ref_lr_gurobi)\n",
    "\n",
    "def ref_gurobi(pa):\n",
    "        \n",
    "    nw_len_lst=np.zeros(pa.simu_len, dtype=int)\n",
    "    nw_res_lst=np.zeros((pa.simu_len,pa.num_res), dtype=int)\n",
    "    for i in range(pa.simu_len):\n",
    "        if np.random.rand() < pa.new_job_rate:  # a new job comes，在每个工作的位置上随机生成一个数字，如果小于新工作率，则在当前位置新生成一个任务\n",
    "            dist=job_distribution.Dist(pa.num_res,pa.max_job_size,pa.max_job_len)\n",
    "            nw_len_lst[i], nw_res_lst[i, :] = dist.bi_model_dist()\n",
    "\n",
    "    T=[]\n",
    "    T_period=pa.max_job_len*pa.simu_len\n",
    "    for i in range(T_period):\n",
    "        T.append(i)\n",
    "\n",
    "    arrval_time_lst=np.zeros(pa.simu_len,dtype=int)\n",
    "\n",
    "    jobs=[]\n",
    "    for i in range(pa.simu_len):\n",
    "        jobs.append(str(i+1))\n",
    "\n",
    "    dauer_dict={}\n",
    "    arrval_dict={}\n",
    "    res_dict={}\n",
    "    i=0\n",
    "    for job in jobs:\n",
    "        dauer_dict[job]=nw_len_lst[i]\n",
    "        arrval_dict[job]=arrval_time_lst[i]\n",
    "        res_dict[job]=nw_res_lst[i]\n",
    "        i=i+1\n",
    "    \n",
    "    m=Model()\n",
    "    m.Params.OutputFlag=0\n",
    "    x=m.addVars(jobs,T,name='start_time_bool',vtype=GRB.BINARY)\n",
    "    omiga=m.addVars(jobs,lb=0,ub=T_period,name='end_time',vtype=GRB.INTEGER)\n",
    "    alpha=m.addVars(jobs,lb=0,ub=T_period,name='start_time',vtype=GRB.INTEGER)\n",
    "    y=m.addVars(jobs,T,name='dauer_time_bool',vtype=GRB.BINARY)\n",
    "    slowdown=m.addVars(jobs,lb=0,name='slow_down',vtype=GRB.INTEGER)\n",
    "        \n",
    "    for job in jobs:\n",
    "        if dauer_dict[job]==0:\n",
    "            m.addConstr(slowdown[job]==1)\n",
    "        else:\n",
    "            m.addConstr(slowdown[job]==(omiga[job]-arrval_dict[job]))\n",
    "            \n",
    "    m.addConstrs((alpha[job]==omiga[job]-dauer_dict[job] for job in jobs))\n",
    "        \n",
    "    for i in range(pa.simu_len):\n",
    "        m.addConstrs(alpha[job]>=arrval_dict[job] for job in jobs)\n",
    "            \n",
    "    for job in jobs:\n",
    "        m.addConstr(quicksum(x[(job),time] for time in T)==1)\n",
    "        m.addConstr(quicksum(y[(job),time] for time in T)==dauer_dict[job])\n",
    "        m.addConstr(quicksum(x[(job),time]*time for time in T)==alpha[job]) \n",
    "        for timee in T[:T_period-max(dauer_dict.values())]:\n",
    "            m.addGenConstrIndicator(x[job,timee],True,quicksum(y[job,timee+timeee] for timeee in range(dauer_dict[job])),GRB.EQUAL,dauer_dict[job])\n",
    "        \n",
    "    for i in range(len(T)):\n",
    "        for j in range(pa.num_res):\n",
    "            m.addConstr(quicksum(y[(job),T[i]]*res_dict[job][j] for job in jobs)<=pa.res_slot)\n",
    "                \n",
    "    m.setObjective(1/pa.simu_len*quicksum(slowdown[job] for job in jobs),GRB.MINIMIZE)\n",
    "        \n",
    "    m.optimize()\n",
    "\n",
    "    \n",
    "    return m.ObjVal\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def main():\n",
    "\n",
    "    import parameters\n",
    "\n",
    "    pa = parameters.Parameters()\n",
    "\n",
    "    pa.simu_len = 40  # 1000\n",
    "    pa.num_ex = 10  #50 # 100\n",
    "    pa.num_nw = pa.simu_len\n",
    "    pa.num_seq_per_batch = 10 #20\n",
    "    pa.output_freq = 10 #50\n",
    "    pa.batch_size = 10\n",
    "\n",
    "    # pa.max_nw_size = 5\n",
    "    # pa.job_len = 5\n",
    "    pa.new_job_rate = 1\n",
    "    \n",
    "\n",
    "    pa.episode_max_length = 2000  # 2000\n",
    "\n",
    "    pa.compute_dependent_parameters()\n",
    "\n",
    "    pg_resume = None\n",
    "    # pg_resume = 'data/tmp_450.pkl'\n",
    "\n",
    "    render = False\n",
    "\n",
    "    launch(pa, pg_resume, render, repre='image', end='all_done')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

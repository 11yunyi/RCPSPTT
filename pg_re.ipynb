{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import environment\n",
    "import job_distribution\n",
    "import slow_down_cdf\n",
    "import RL_brain\n",
    "import parameters\n",
    "from gurobipy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Given vector x, computes a vector y such that\n",
    "    y[i] = x[i] + gamma * x[i+1] + gamma^2 x[i+2] + ...\n",
    "    \"\"\"\n",
    "    out = np.zeros(len(x))\n",
    "    out[-1] = x[-1]\n",
    "    for i in reversed(range(len(x)-1)):\n",
    "        out[i] = x[i] + gamma*out[i+1]\n",
    "    assert x.ndim >= 1\n",
    "    # More efficient version:\n",
    "    # scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj(agent, env, episode_max_length):\n",
    "    \"\"\"\n",
    "    Run agent-environment loop for one whole episode (trajectory)\n",
    "    Return dictionary of results\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    obs = []\n",
    "    acts = []\n",
    "    rews = []\n",
    "    info = []\n",
    "\n",
    "    ob = env.observe()\n",
    "\n",
    "    for _ in range(episode_max_length):\n",
    "    #for _ in range(5):\n",
    "\n",
    "        loss = 0\n",
    "        #print('ob_len:',len(ob[0]))\n",
    "        a = agent.choose_action(ob)\n",
    "\n",
    "        obs.append(ob)  # store the ob at current decision making step\n",
    "        acts.append(a)\n",
    "\n",
    "        ob_, rew, done, info = env.step(a, repeat=True)\n",
    "\n",
    "        # agent.store_transition(ob, a, rew)\n",
    "\n",
    "        rews.append(rew)\n",
    "\n",
    "        if done:\n",
    "\n",
    "            # loss = agent.learn()\n",
    "            break\n",
    "\n",
    "        ob = ob_\n",
    "\n",
    "    # loss = agent.learn()\n",
    "\n",
    "    return {'reward': np.array(rews),\n",
    "            'ob': np.array(obs),\n",
    "            'action': np.array(acts),\n",
    "            'info': info,\n",
    "            # 'loss': loss\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_all_ob(trajs, pa):\n",
    "\n",
    "    timesteps_total = 0\n",
    "    for i in range(len(trajs)):\n",
    "        timesteps_total += len(trajs[i]['reward'])\n",
    "\n",
    "    all_ob = np.zeros(\n",
    "        (timesteps_total, pa.network_input_height*pa.network_input_width),\n",
    "        dtype=np.float64)\n",
    "\n",
    "    timesteps = 0\n",
    "    for i in range(len(trajs)):\n",
    "        for j in range(len(trajs[i]['reward'])):\n",
    "            all_ob[timesteps, :] = trajs[i]['ob'][j]\n",
    "            timesteps += 1\n",
    "\n",
    "    return all_ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_info(trajs):\n",
    "    enter_time = []\n",
    "    finish_time = []\n",
    "    job_len = []\n",
    "\n",
    "    for traj in trajs:\n",
    "        enter_time.append(np.array([traj['info'].record[i].enter_time for i in range(len(traj['info'].record))]))\n",
    "        finish_time.append(np.array([traj['info'].record[i].finish_time for i in range(len(traj['info'].record))]))\n",
    "        job_len.append(np.array([traj['info'].record[i].len for i in range(len(traj['info'].record))]))\n",
    "\n",
    "    enter_time = np.concatenate(enter_time)\n",
    "    finish_time = np.concatenate(finish_time)\n",
    "    job_len = np.concatenate(job_len)\n",
    "\n",
    "    return enter_time, finish_time, job_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_curve(output_file_prefix, max_rew_lr_curve, mean_rew_lr_curve, slow_down_lr_curve,\n",
    "                  ref_discount_rews, ref_slow_down,ref_lr_gurobi=None):\n",
    "    num_colors = len(ref_discount_rews) + 2\n",
    "    cm = plt.get_cmap('gist_rainbow')\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "    #ax.set_prop_cycle([cm(1. * i / num_colors) for i in range(num_colors)])\n",
    "    ax.set_prop_cycle(color=['blue', 'green', 'orange', 'red','yellow','gray'])\n",
    "    ax.plot(mean_rew_lr_curve, linewidth=2, label='PG mean')\n",
    "    ax.plot(max_rew_lr_curve, linewidth=2, label='PG max')\n",
    "    for k in ref_discount_rews:\n",
    "        ax.plot(np.tile(np.average(ref_discount_rews[k]), len(mean_rew_lr_curve)), linewidth=2, label=k)\n",
    "    plt.legend(loc=4)\n",
    "    plt.xlabel(\"Iteration\", fontsize=20)\n",
    "    plt.ylabel(\"Discounted Total Reward\", fontsize=20)\n",
    "\n",
    "    \n",
    "    \n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_prop_cycle(color=['blue', 'green', 'orange', 'red','yellow','gray'])\n",
    "    #ax.set_color_cycle([cm(1. * i / num_colors) for i in range(num_colors)])\n",
    "\n",
    "    ax.plot(slow_down_lr_curve, linewidth=2, label='PG mean')\n",
    "    #######todo：添加gurobi计算结果####\n",
    "    #ax.plot(ref_lr_gurobi, linewidth=2, label='Gurobi')\n",
    "    ###########################\n",
    "    for k in ref_discount_rews:\n",
    "        ax.plot(np.tile(np.average(np.concatenate(ref_slow_down[k])), len(slow_down_lr_curve)), linewidth=2, label=k)\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel(\"Iteration\", fontsize=20)\n",
    "    plt.ylabel(\"Completion time\", fontsize=20)\n",
    "\n",
    "    plt.savefig(output_file_prefix + \"_lr_curve\" + \".pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traj_worker(rl, env, pa):\n",
    "\n",
    "    trajs = []\n",
    "\n",
    "    for i in range(pa.num_seq_per_batch):\n",
    "        traj = get_traj(rl, env, pa.episode_max_length)\n",
    "        trajs.append(traj)\n",
    "\n",
    "    all_ob = concatenate_all_ob(trajs, pa)\n",
    "\n",
    "    # Compute discounted sums of rewards\n",
    "    rets = [discount(traj[\"reward\"], pa.discount) for traj in trajs]\n",
    "    maxlen = max(len(ret) for ret in rets)\n",
    "    padded_rets = [np.concatenate([ret, np.zeros(maxlen - len(ret))]) for ret in rets]\n",
    "\n",
    "    # Compute time-dependent baseline\n",
    "    baseline = np.mean(padded_rets, axis=0)\n",
    "\n",
    "    # Compute advantage function\n",
    "    advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "    all_action = np.concatenate([traj[\"action\"] for traj in trajs])\n",
    "    all_adv = np.concatenate(advs)\n",
    "\n",
    "    all_eprews = np.array([discount(traj[\"reward\"], pa.discount)[0] for traj in trajs])  # episode total rewards\n",
    "    all_eplens = np.array([len(traj[\"reward\"]) for traj in trajs])  # episode lengths\n",
    "    # all_loss = np.array([traj[\"loss\"] for traj in trajs])\n",
    "\n",
    "    # All Job Stat\n",
    "    enter_time, finish_time, job_len = process_all_info(trajs)\n",
    "    finished_idx = (finish_time >= 0)\n",
    "    completion_time = finish_time[finished_idx] - enter_time[finished_idx]\n",
    "\n",
    "    return all_eprews, all_eplens, completion_time, all_ob, all_action, all_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for workers...\n",
      "-prepare for env- 0\n",
      "-prepare for env- 1\n",
      "-prepare for env- 2\n",
      "-prepare for env- 3\n",
      "-prepare for env- 4\n",
      "-prepare for env- 5\n",
      "-prepare for env- 6\n",
      "-prepare for env- 7\n",
      "-prepare for env- 8\n",
      "-prepare for env- 9\n",
      "-prepare for worker-\n",
      "WARNING:tensorflow:From /Users/11yunyi/Desktop/Diplomarbeit/PG_for_cloud/PG_ConV for completion time/RL_brain.py:50: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /Users/11yunyi/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/11yunyi/Desktop/Diplomarbeit/PG_for_cloud/PG_ConV for completion time/RL_brain.py:53: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From /Users/11yunyi/Desktop/Diplomarbeit/PG_for_cloud/PG_ConV for completion time/RL_brain.py:70: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Preparing for reference data...\n",
      "Load on # 0 resource dimension is 2.7462222222222223\n",
      "Load on # 1 resource dimension is 3.1927777777777777\n",
      "\n",
      "\n",
      "\n",
      "=============== 0 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -12349.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -5647.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -11216.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -10420.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 1 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -8559.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -3915.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -7819.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -9070.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 2 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -7684.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -2661.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -7245.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -7505.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 3 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -10726.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -4524.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -8913.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -9967.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 4 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -8196.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -3786.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -7307.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -8571.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 5 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -12452.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -4535.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -11370.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -10012.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 6 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -9381.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -3240.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -8917.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -9833.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 7 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -7371.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -3578.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -7562.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -9141.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 8 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -10635.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -3497.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -10026.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -8799.0\n",
      "\n",
      "\n",
      "\n",
      "=============== 9 ===============\n",
      "---------- Tetris -----------\n",
      "total discount reward : \t -8447.0\n",
      "---------- SJF -----------\n",
      "total discount reward : \t -3909.0\n",
      "---------- packer -----------\n",
      "total discount reward : \t -7742.0\n",
      "---------- Random -----------\n",
      "total discount reward : \t -9451.0\n",
      "Start training...\n"
     ]
    }
   ],
   "source": [
    "def launch(pa, pg_resume=None, render=False, repre='image', end='no_new_job'):\n",
    "\n",
    "    # ----------------------------\n",
    "    print(\"Preparing for workers...\")\n",
    "    # ----------------------------\n",
    "\n",
    "    pg_learners = []\n",
    "    envs = []\n",
    "\n",
    "    nw_len_seqs, nw_size_seqs = job_distribution.generate_sequence_work(pa, seed=42)#生成一序列的任务，其中包括num_ex个task\n",
    "\n",
    "    for ex in range(pa.num_ex):#对于每个task\n",
    "\n",
    "        print(\"-prepare for env-\", ex)\n",
    "\n",
    "        env = environment.Env(pa, nw_len_seqs=nw_len_seqs, nw_size_seqs=nw_size_seqs,\n",
    "                              render=False, repre=repre, end=end)#初始化一个环境\n",
    "        env.seq_no = ex\n",
    "        envs.append(env)\n",
    "\n",
    "    print(\"-prepare for worker-\")\n",
    "\n",
    "    rl = RL_brain.PolicyGradient(n_actions=pa.network_output_dim,\n",
    "                                 network_input_height=pa.network_input_height,\n",
    "                                 network_input_width=pa.network_input_width,\n",
    "                                 n_features=pa.network_input_height*pa.network_input_width,\n",
    "                                 learning_rate=0.002)\n",
    "\n",
    "\n",
    "    if pg_resume is not None:\n",
    "        rl.load_data(pg_resume)\n",
    "\n",
    "\n",
    "    # --------------------------------------\n",
    "    print(\"Preparing for reference data...\")\n",
    "    # --------------------------------------\n",
    "\n",
    "    ref_discount_rews, ref_slow_down = slow_down_cdf.launch(pa, pg_resume=None,render=True,\n",
    "                                                            plot=False, repre=repre, end=end)\n",
    "    mean_rew_lr_curve = []\n",
    "    max_rew_lr_curve = []\n",
    "    slow_down_lr_curve = []\n",
    "\n",
    "    # --------------------------------------\n",
    "    print(\"Start training...\")\n",
    "    # --------------------------------------\n",
    "\n",
    "    timer_start = time.time()\n",
    "    \n",
    "    ref_lr_gurobi=[]\n",
    "\n",
    "    for iteration in range(1, pa.num_epochs):#进行每一次迭代\n",
    "    #for iteration in range(1, 2):\n",
    "\n",
    "        ex_indices = list(range(pa.num_ex))\n",
    "        np.random.shuffle(ex_indices)#打乱每一次的所有task\n",
    "\n",
    "        all_eprews = []#所有迭代次数的rewards\n",
    "        eprews = []#每次迭代的总rewards\n",
    "        eplens = []#每次迭代完成所有任务的总时长\n",
    "        all_slowdown = []#所有迭代的总slowdown\n",
    "\n",
    "        eprewlist = []\n",
    "        eplenlist =[]\n",
    "        slowdownlist =[]\n",
    "        losslist = []\n",
    "        \n",
    "\n",
    "        ex_counter = 0\n",
    "        for ex in range(pa.num_ex):\n",
    "\n",
    "            ex_idx = ex_indices[ex]\n",
    "\n",
    "            eprew, eplen, completion_time, all_ob, all_action, all_adv = get_traj_worker(rl, envs[ex_idx], pa)\n",
    "            eprewlist.append(eprew)\n",
    "            eplenlist.append(eplen)\n",
    "            slowdownlist.append(completion_time)\n",
    "            \n",
    "            \n",
    "            loss = rl.learn(all_ob, all_action, all_adv)\n",
    "            losslist.append(loss)\n",
    "\n",
    "            ex_counter += 1\n",
    "\n",
    "            if ex_counter >= pa.batch_size or ex == pa.num_ex - 1:\n",
    "\n",
    "                print(\"\\n\\n\")\n",
    "\n",
    "                ex_counter = 0\n",
    "\n",
    "               \n",
    "\n",
    "        timer_end = time.time()\n",
    "\n",
    "        print(\"-----------------\")\n",
    "        print(\"Iteration: \\t %i\" % iteration)\n",
    "        print(\"NumTrajs: \\t %i\" % len(eprewlist))\n",
    "        print(\"NumTimesteps: \\t %i\" % np.sum(eplenlist))\n",
    "        print(\"Loss:     \\t %s\" % np.mean(losslist))\n",
    "        print(\"MaxRew: \\t %s\" % np.average([np.max(rew) for rew in eprewlist]))\n",
    "        print(\"MeanRew: \\t %s +- %s\" % (np.mean(eprewlist), np.std(eprewlist)))\n",
    "        print(\"MeanSlowdown: \\t %s\" % np.mean([np.mean(sd) for sd in slowdownlist]))\n",
    "        print(\"MeanLen: \\t %s +- %s\" % (np.mean(eplenlist), np.std(eplenlist)))\n",
    "        print(\"Elapsed time\\t %s\" % (timer_end - timer_start), \"seconds\")\n",
    "        print(\"-----------------\")\n",
    "\n",
    "        timer_start = time.time()\n",
    "\n",
    "        max_rew_lr_curve.append(np.average([np.max(rew) for rew in eprewlist]))\n",
    "        mean_rew_lr_curve.append(np.mean(eprewlist))\n",
    "        slow_down_lr_curve.append(np.mean([np.mean(sd) for sd in slowdownlist]))\n",
    "        \n",
    "        #slowdown_gurobi=ref_gurobi(pa)\n",
    "        #ref_lr_gurobi.append(slowdown_gurobi)\n",
    "        \n",
    "        \n",
    "        if iteration % pa.output_freq == 0:\n",
    "\n",
    "            rl.save_data(pa.output_filename + '_' + str(iteration))\n",
    "\n",
    "            pa.unseen = True\n",
    "            slow_down_cdf.launch(pa, pa.output_filename + '_' + str(iteration) + '.ckpt',render=False, plot=True, repre=repre, end=end)\n",
    "            pa.unseen = False\n",
    "            #print(slow_down_lr_curve)\n",
    "            #mean_lr_gurobi=np.mean(ref_lr_gurobi)\n",
    "            #for i in range(len(ref_lr_gurobi)):\n",
    "                #ref_lr_gurobi[i]=mean_lr_gurobi\n",
    "\n",
    "            plot_lr_curve(pa.output_filename,\n",
    "                          max_rew_lr_curve, mean_rew_lr_curve, slow_down_lr_curve,\n",
    "                          ref_discount_rews, ref_slow_down,ref_lr_gurobi)\n",
    "\n",
    "def ref_gurobi(pa):\n",
    "        \n",
    "    nw_len_lst=np.zeros(pa.simu_len, dtype=int)\n",
    "    nw_res_lst=np.zeros((pa.simu_len,pa.num_res), dtype=int)\n",
    "    for i in range(pa.simu_len):\n",
    "        if np.random.rand() < pa.new_job_rate:  # a new job comes，在每个工作的位置上随机生成一个数字，如果小于新工作率，则在当前位置新生成一个任务\n",
    "            dist=job_distribution.Dist(pa.num_res,pa.max_job_size,pa.max_job_len)\n",
    "            nw_len_lst[i], nw_res_lst[i, :] = dist.bi_model_dist()\n",
    "\n",
    "    T=[]\n",
    "    T_period=pa.max_job_len*pa.simu_len\n",
    "    for i in range(T_period):\n",
    "        T.append(i)\n",
    "\n",
    "    arrval_time_lst=np.zeros(pa.simu_len,dtype=int)\n",
    "\n",
    "    jobs=[]\n",
    "    for i in range(pa.simu_len):\n",
    "        jobs.append(str(i+1))\n",
    "\n",
    "    dauer_dict={}\n",
    "    arrval_dict={}\n",
    "    res_dict={}\n",
    "    i=0\n",
    "    for job in jobs:\n",
    "        dauer_dict[job]=nw_len_lst[i]\n",
    "        arrval_dict[job]=arrval_time_lst[i]\n",
    "        res_dict[job]=nw_res_lst[i]\n",
    "        i=i+1\n",
    "    \n",
    "    m=Model()\n",
    "    m.Params.OutputFlag=0\n",
    "    x=m.addVars(jobs,T,name='start_time_bool',vtype=GRB.BINARY)\n",
    "    omiga=m.addVars(jobs,lb=0,ub=T_period,name='end_time',vtype=GRB.INTEGER)\n",
    "    alpha=m.addVars(jobs,lb=0,ub=T_period,name='start_time',vtype=GRB.INTEGER)\n",
    "    y=m.addVars(jobs,T,name='dauer_time_bool',vtype=GRB.BINARY)\n",
    "    slowdown=m.addVars(jobs,lb=0,name='slow_down',vtype=GRB.INTEGER)\n",
    "        \n",
    "    for job in jobs:\n",
    "        if dauer_dict[job]==0:\n",
    "            m.addConstr(slowdown[job]==1)\n",
    "        else:\n",
    "            m.addConstr(slowdown[job]==(omiga[job]-arrval_dict[job]))\n",
    "            \n",
    "    m.addConstrs((alpha[job]==omiga[job]-dauer_dict[job] for job in jobs))\n",
    "        \n",
    "    for i in range(pa.simu_len):\n",
    "        m.addConstrs(alpha[job]>=arrval_dict[job] for job in jobs)\n",
    "            \n",
    "    for job in jobs:\n",
    "        m.addConstr(quicksum(x[(job),time] for time in T)==1)\n",
    "        m.addConstr(quicksum(y[(job),time] for time in T)==dauer_dict[job])\n",
    "        m.addConstr(quicksum(x[(job),time]*time for time in T)==alpha[job]) \n",
    "        for timee in T[:T_period-max(dauer_dict.values())]:\n",
    "            m.addGenConstrIndicator(x[job,timee],True,quicksum(y[job,timee+timeee] for timeee in range(dauer_dict[job])),GRB.EQUAL,dauer_dict[job])\n",
    "        \n",
    "    for i in range(len(T)):\n",
    "        for j in range(pa.num_res):\n",
    "            m.addConstr(quicksum(y[(job),T[i]]*res_dict[job][j] for job in jobs)<=pa.res_slot)\n",
    "                \n",
    "    m.setObjective(1/pa.simu_len*quicksum(slowdown[job] for job in jobs),GRB.MINIMIZE)\n",
    "        \n",
    "    m.optimize()\n",
    "\n",
    "    \n",
    "    return m.ObjVal\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def main():\n",
    "\n",
    "    import parameters\n",
    "\n",
    "    pa = parameters.Parameters()\n",
    "\n",
    "    pa.simu_len = 90  # 1000\n",
    "    pa.num_ex = 10  #50 # 100\n",
    "    pa.num_nw = pa.simu_len\n",
    "    pa.num_seq_per_batch = 10 #20\n",
    "    pa.output_freq = 50 #50\n",
    "    pa.batch_size = 10\n",
    "\n",
    "    # pa.max_nw_size = 5\n",
    "    # pa.job_len = 5\n",
    "    pa.new_job_rate = 1\n",
    "    \n",
    "\n",
    "    pa.episode_max_length = 2000  # 2000\n",
    "\n",
    "    pa.compute_dependent_parameters()\n",
    "\n",
    "    pg_resume = None\n",
    "    # pg_resume = 'data/tmp_450.pkl'\n",
    "\n",
    "    render = False\n",
    "\n",
    "    launch(pa, pg_resume, render, repre='image', end='all_done')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import environment\n",
    "import parameters\n",
    "# import pg_network\n",
    "import other_agents\n",
    "import RL_brain\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PG', 'Tetris', 'SJF', 'packer', 'Random']\n",
      "Load on # 0 resource dimension is 2.4625\n",
      "Load on # 1 resource dimension is 2.86\n",
      "\n",
      "\n",
      "\n",
      "=============== 0 ===============\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: data/tem_10.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f77bffa5b8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f77bffa5b8be>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0mall_discount_rews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs_slow_down\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepre\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all_done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_discount_rews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjobs_slow_down\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f77bffa5b8be>\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(pa, pg_resume, render, plot, repre, end)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mrews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_traj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------- \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" -----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f77bffa5b8be>\u001b[0m in \u001b[0;36mget_traj\u001b[0;34m(test_type, pa, env, episode_max_length, pg_resume, render)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                      \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_input_width\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_input_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                      learning_rate=0.02)#初始化一个PG的agent\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# net_handle = open(pg_resume, 'rb')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Diplomarbeit/PG_for_cloud/PG_ConV for completion time/RL_brain.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, pg_resume)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[0;32m-> 1278\u001b[0;31m                        compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: data/tem_10.ckpt"
     ]
    }
   ],
   "source": [
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Given vector x, computes a vector y such that\n",
    "    y[i] = x[i] + gamma * x[i+1] + gamma^2 x[i+2] + ...\n",
    "    \"\"\"\n",
    "    out = np.zeros(len(x))\n",
    "    out[-1] = x[-1]\n",
    "    for i in reversed(range(len(x)-1)):\n",
    "        out[i] = x[i] + gamma*out[i+1]\n",
    "    assert x.ndim >= 1\n",
    "    # More efficient version:\n",
    "    # scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def categorical_sample(prob_n):\n",
    "    \"\"\"\n",
    "    Sample from categorical distribution,\n",
    "    specified by a vector of class probabilities\n",
    "    \"\"\"\n",
    "    prob_n = np.asarray(prob_n)\n",
    "    csprob_n = np.cumsum(prob_n)\n",
    "    return (csprob_n > np.random.rand()).argmax()\n",
    "\n",
    "\n",
    "def get_traj(test_type, pa, env, episode_max_length, pg_resume=None, render=False):\n",
    "    \"\"\"\n",
    "    Run agent-environment loop for one whole episode (trajectory)\n",
    "    Return dictionary of results\n",
    "    \"\"\"\n",
    "\n",
    "    if test_type == 'PG':  # load trained parameters\n",
    "        tf.reset_default_graph()\n",
    "        # pg_learner = pg_network.PGLearner(pa)\n",
    "        rl = RL_brain.PolicyGradient(n_actions=pa.network_output_dim,\n",
    "                                     network_input_width=pa.network_input_width,\n",
    "                                     network_input_height=pa.network_input_height,\n",
    "                                     n_features=pa.network_input_width * pa.network_input_height,\n",
    "                                     learning_rate=0.02)#初始化一个PG的agent\n",
    "        rl.load_data(pg_resume)\n",
    "\n",
    "        # net_handle = open(pg_resume, 'rb')\n",
    "        # net_params = pickle.load(net_handle)\n",
    "        # pg_learner.set_net_params(net_params)\n",
    "\n",
    "    env.reset()\n",
    "    rews = []\n",
    "\n",
    "    ob = env.observe()#获得现在环境的观察值\n",
    "\n",
    "    for _ in range(episode_max_length):#在当前观察值下选择动作\n",
    "\n",
    "        if test_type == 'PG':\n",
    "            a = rl.choose_action(ob)\n",
    "\n",
    "        elif test_type == 'Tetris':\n",
    "            a = other_agents.get_packer_action(env.machine, env.job_slot)\n",
    "\n",
    "        elif test_type == 'SJF':\n",
    "            a = other_agents.get_sjf_action(env.machine, env.job_slot)\n",
    "\n",
    "        elif test_type == 'Random':\n",
    "            a = other_agents.get_random_action(env.job_slot)\n",
    "        \n",
    "        elif test_type == 'packer':\n",
    "            a = other_agents.get_packer_sjf_action(env.machine, env.job_slot,0.05)\n",
    "\n",
    "        ob, rew, done, info = env.step(a, repeat=True)#执行一个动作，获得执行完这个动作之后的观测值\n",
    "\n",
    "        rews.append(rew)##把所有的单步奖励添加到rews里\n",
    "\n",
    "        if done: break#如果单个task结束，跳出循环\n",
    "        if render: env.render()\n",
    "        # env.render()\n",
    "\n",
    "    return np.array(rews), info#返回这个episode的奖励轨迹和执行的job轨迹\n",
    "\n",
    "\n",
    "def launch(pa, pg_resume=None, render=False, plot=False, repre='image', end='no_new_job'):\n",
    "\n",
    "    # ---- Parameters ----\n",
    "\n",
    "    test_types = ['Tetris', 'SJF','packer','Random']#测试的类型分为Tetris，SJF和packer\n",
    "\n",
    "    if pg_resume is not None:\n",
    "        test_types = ['PG'] + test_types\n",
    "        print(test_types)\n",
    "    env = environment.Env(pa, render, repre=repre, end=end)#初始化一个环境\n",
    "\n",
    "    all_discount_rews = {}\n",
    "    jobs_slow_down = {}\n",
    "    work_complete = {}\n",
    "    work_remain = {}\n",
    "    job_len_remain = {}\n",
    "    num_job_remain = {}\n",
    "    job_remain_delay = {}\n",
    "\n",
    "    for test_type in test_types:\n",
    "        all_discount_rews[test_type] = []\n",
    "        jobs_slow_down[test_type] = []\n",
    "        work_complete[test_type] = []\n",
    "        work_remain[test_type] = []\n",
    "        job_len_remain[test_type] = []\n",
    "        num_job_remain[test_type] = []\n",
    "        job_remain_delay[test_type] = []\n",
    "\n",
    "    for seq_idx in range(pa.num_ex):\n",
    "        #一组有pa.num_ex数量的序列，每个序列代表一整个task（一整个task就是，比如要完成50个任务的安排）\n",
    "        print('\\n\\n')\n",
    "        print(\"=============== \" + str(seq_idx) + \" ===============\")\n",
    "\n",
    "        for test_type in test_types:\n",
    "\n",
    "            rews, info = get_traj(test_type, pa, env, pa.episode_max_length, pg_resume)\n",
    "\n",
    "            print(\"---------- \" + test_type + \" -----------\")\n",
    "\n",
    "            print(\"total discount reward : \\t %s\" % (discount(rews, pa.discount)[0]))\n",
    "\n",
    "            all_discount_rews[test_type].append(\n",
    "                discount(rews, pa.discount)[0]\n",
    "            )\n",
    "\n",
    "            # ------------------------\n",
    "            # ---- per job stat ----\n",
    "            # ------------------------\n",
    "\n",
    "            enter_time = np.array([info.record[i].enter_time for i in range(len(info.record))])\n",
    "            #print('enter_time',enter_time)\n",
    "            finish_time = np.array([info.record[i].finish_time for i in range(len(info.record))])\n",
    "            job_len = np.array([info.record[i].len for i in range(len(info.record))])\n",
    "            job_total_size = np.array([np.sum(info.record[i].res_vec) for i in range(len(info.record))])\n",
    "            #print('finish_time',finish_time)\n",
    "            finished_idx = (finish_time >= 0)\n",
    "            #print('finish_idx',finished_idx)\n",
    "            unfinished_idx = (finish_time < 0)\n",
    "\n",
    "            jobs_slow_down[test_type].append(\n",
    "                (finish_time[finished_idx] - enter_time[finished_idx])\n",
    "            )\n",
    "            work_complete[test_type].append(\n",
    "                np.sum(job_len[finished_idx] * job_total_size[finished_idx])\n",
    "            )\n",
    "            work_remain[test_type].append(\n",
    "                np.sum(job_len[unfinished_idx] * job_total_size[unfinished_idx])\n",
    "            )\n",
    "            job_len_remain[test_type].append(\n",
    "                np.sum(job_len[unfinished_idx])\n",
    "            )\n",
    "            num_job_remain[test_type].append(\n",
    "                len(job_len[unfinished_idx])\n",
    "            )\n",
    "            job_remain_delay[test_type].append(\n",
    "                np.sum(pa.episode_max_length - enter_time[unfinished_idx])\n",
    "            )\n",
    "\n",
    "        env.seq_no = (env.seq_no + 1) % env.pa.num_ex\n",
    "\n",
    "    # -- matplotlib colormap no overlap --\n",
    "    if plot:\n",
    "        num_colors = len(test_types)\n",
    "        cm = plt.get_cmap('gist_rainbow')\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_prop_cycle(color=['grey', 'purple', 'blue', 'green', 'orange', 'red'])\n",
    "        #ax.set_color_cycle([cm(1. * i / num_colors) for i in range(num_colors)])\n",
    "\n",
    "        for test_type in test_types:\n",
    "            slow_down_cdf = np.sort(np.concatenate(jobs_slow_down[test_type]))\n",
    "            slow_down_yvals = np.arange(len(slow_down_cdf))/float(len(slow_down_cdf))\n",
    "            ax.plot(slow_down_cdf, slow_down_yvals, linewidth=2, label=test_type)\n",
    "\n",
    "        plt.legend(loc=4)\n",
    "        plt.xlabel(\"Completion time\", fontsize=20)\n",
    "        plt.ylabel(\"CDF\", fontsize=20)\n",
    "        plt.show()\n",
    "        #plt.savefig(pg_resume + \"_slowdown_fig\" + \".pdf\")\n",
    "\n",
    "    return all_discount_rews, jobs_slow_down\n",
    "\n",
    "\n",
    "def main():\n",
    "    pa = parameters.Parameters()\n",
    "\n",
    "    pa.simu_len = 20# 5000  # 1000\n",
    "    pa.num_ex = 10  # 100\n",
    "    pa.num_nw = pa.simu_len\n",
    "    pa.num_seq_per_batch = 20\n",
    "    # pa.max_nw_size = 5\n",
    "    # pa.job_len = 5\n",
    "    pa.new_job_rate = 1\n",
    "    pa.discount = 1\n",
    "\n",
    "    pa.episode_max_length = 20000  # 2000\n",
    "\n",
    "    pa.compute_dependent_parameters()\n",
    "\n",
    "    render = True\n",
    "\n",
    "    plot = True  # plot slowdown cdf\n",
    "\n",
    "    #pg_resume = None\n",
    "    #pg_resume = 'data/pg_re_discount_1_rate_0.3_simu_len_200_num_seq_per_batch_20_ex_10_nw_10_1450.pkl'\n",
    "    #pg_resume = 'data/pg_re_1000_discount_1_5990.pkl'\n",
    "    pg_resume='data/tem_10.ckpt'\n",
    "    pa.unseen = True\n",
    "\n",
    "    all_discount_rews, jobs_slow_down=launch(pa, pg_resume, render, plot, repre='image', end='all_done')\n",
    "    for k in all_discount_rews:\n",
    "        print(np.average(np.concatenate(jobs_slow_down[k])))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The passed save_path is not a valid checkpoint: data/tem_10.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8fe573d61c26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 learning_rate=0.02)#初始化一个PG的agent\n\u001b[1;32m      9\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/tem_10.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Diplomarbeit/PG_for_cloud/PG_ConV for completion time/RL_brain.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, pg_resume)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpg_resume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m       raise ValueError(\"The passed save_path is not a valid checkpoint: \" +\n\u001b[0;32m-> 1278\u001b[0;31m                        compat.as_text(save_path))\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The passed save_path is not a valid checkpoint: data/tem_10.ckpt"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# pg_learner = pg_network.PGLearner(pa)\n",
    "pa=parameters.Parameters()\n",
    "rl = RL_brain.PolicyGradient(n_actions=pa.network_output_dim,\n",
    "                                network_input_width=pa.network_input_width,\n",
    "                                network_input_height=pa.network_input_height,\n",
    "                                n_features=pa.network_input_width * pa.network_input_height,\n",
    "                                learning_rate=0.02)#初始化一个PG的agent\n",
    "pg_resume='data/tem_10.ckpt'\n",
    "rl.load_data(pg_resume)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
